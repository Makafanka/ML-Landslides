# -*- coding: utf-8 -*-
"""spatial_join_example.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yypHVKLyyt7qcXOz-9CPBlBk6yEdWa9N
"""

from google.colab import drive
drive.mount('/content/drive')

import datetime as dt
import geopandas as gpd
# import networkx as nx
import numpy as np
import pandas as pd
from shapely.geometry import Point
from shapely.geometry import LineString

# spatial join between points and polylines
def spatialJoin(radius, input_data, network):

    # only deal with the unpaired data
    # cols = ['zone_id', 'display_name', 'state', 'road', 'direction', 'location_description', 'lane_type', 'organization', 'detector_type', 'latitude', 'longitude', 'bearing', 'default_speed', 'interval', 'length']
    cols = ['Event ID', 'Start time', 'Closed time', 'Location', 'Latitude', 'Longitude', 'Road', 'Direction', 'County', 'State', 'Roadway Clearance Time', 'Duration (Incident clearance time)', 'Year', 'Month']
    # input_data_todo = input_data[input_data.LINKID.isna()][cols].copy()
    input_data_todo = input_data[cols].copy()
    print('todo input data shape', input_data_todo.shape)

    # convert points to circles
    input_data_todo = gpd.GeoDataFrame(
        input_data_todo,
        crs={'init': 'epsg:4326'},
        geometry=gpd.points_from_xy(input_data_todo.Longitude, input_data_todo.Latitude))
    input_data_todo_buffer = input_data_todo.to_crs(epsg=3857).buffer(distance=radius, resolution=1).to_crs(epsg=4326)
    input_data_todo['buffer'] = input_data_todo_buffer

    # rename buffer to geometry
    input_data_todo = input_data_todo.rename(columns={'geometry': 'point'})
    input_data_todo = input_data_todo.rename(columns={'buffer': 'geometry'})

    # convert polylines to polygons
    network['geometry'] = network.apply(lambda row: LineString([(row['start_longitude'], row['start_latitude']),
                                                  (row['end_longitude'], row['end_latitude'])]), axis=1)
    network_buffer = network.to_crs(epsg=3857).buffer(distance=radius, resolution=1).to_crs(epsg=4326)
    network['buffer'] = network_buffer

    # rename buffer to geometry
    network = network.rename(columns={'geometry': 'line'})
    network = network.rename(columns={'buffer': 'geometry'})

    # sjoin
    output_data = gpd.sjoin(input_data_todo, network, how='left', predicate='intersects')

    print('sjoin failed:', output_data.shape[0])

    return output_data

file_path = '/content/drive/MyDrive/Landslides_Research/obs_full.geojson'
data = gpd.read_file(file_path)
data

file_path1 = '/content/drive/MyDrive/Landslides_Research/network_tmc_md_.geojson'
file_path2 = '/content/drive/MyDrive/Landslides_Research/flood_full.geojson'

# Read the GeoJSON file
data = gpd.read_file(file_path1)
flood = gpd.read_file(file_path2)

flood

gdf = flood

gdf['Start time'] = pd.to_datetime(gdf['Start time'])
gdf['Closed time'] = pd.to_datetime(gdf['Closed time'])

print(gdf['Start time'].sort_values(ascending=False))


# Define the start and end dates for filtering
start_date = pd.Timestamp('2023-08-01', tz='UTC')
end_date = pd.Timestamp('2023-08-31', tz='UTC')

# Use boolean indexing to filter the geodataframe
filtered_gdf = gdf[(gdf['Start time'] >= start_date) & (gdf['Start time'] <= end_date)]
filtered_gdf

data

# output = spatialJoin(radius=1, input_data=filtered_gdf, network=data)
output = spatialJoin(radius=1, input_data=gdf, network=data)
output['join_successful'] = ~output['index_right'].isna()
a = output[output['join_successful'] == True]
for i in range(2, 33):
    output = spatialJoin(radius=i, input_data=gdf, network=data)
    output['join_successful'] = ~output['index_right'].isna()
    rem = output[output['join_successful'] == True]
    a = gpd.GeoDataFrame(pd.concat([a, rem], ignore_index=True))
    a = a.drop_duplicates(subset=['Event ID'])
print(a)
print(a.shape[0])
output = a

# output = spatialJoin(radius=32, input_data=flood, network=data)
# output['join_successful'] = ~output['index_right'].isna()
# output = output[output['join_successful'] == True]
# print(output)
# print(output.shape[0])
# radius: loop from 1 to 32
# list of overlap
# radius = 1: accurate, delete from dataset
# radius = 2: delete from dataset, output
# ...
# check the cases when there are several roads

output.columns

# Save the GeoDataFrame as a GeoJSON file
output['line'].to_file('drive/MyDrive/Landslides_Research/frontage_joined.geojson', driver='GeoJSON')
output['point'].to_file('drive/MyDrive/Landslides_Research/flood_joined_f.geojson', driver='GeoJSON')

start_date = pd.Timestamp('2013-01-01', tz='UTC')
end_date = pd.Timestamp('2023-08-31', tz='UTC')

# Create a DatetimeIndex with 30-minute frequency
measurement_tstamp = pd.date_range(start=start_date, end=end_date, freq='30T')

# Print the time stamps
print(measurement_tstamp)

# Step 2: Create a dictionary to store flood information
flood_info = {}

# Step 3: Iterate over the floods and mark timestamps as flooded
for index, flood in output.iterrows():
    start_time = flood['Start time']
    closed_time = flood['Closed time']
    tmc = flood['tmc']

    if pd.notna(tmc):  # Skip NaN tmcs
        for timestamp in measurement_tstamp:
            if timestamp not in flood_info:
                flood_info[timestamp] = {}
            if start_time <= timestamp <= closed_time:
                flood_info[timestamp][tmc] = 1

# Step 4: Create a new DataFrame
sorted_items = sorted(flood_info.items())
flood_info = dict(sorted_items)
flood_df = pd.DataFrame(flood_info).T.fillna(0).astype(int)
flood_df = flood_df.reindex(columns=data['tmc'].tolist(), fill_value=0)

flood_df

flood_df = flood_df.fillna(0)

# Find the row and column indices of 1s
row_indices, col_indices = np.where(flood_df == 1)

# Combine row and column indices
positions = list(zip(row_indices, col_indices))

print(positions)

# Extract the value at [334, 917]
center_value = flood_df.iloc[1543, 116]

# Define the range of rows and columns around the center
row_range = range(1540, 1550)
col_range = range(113, 118)

# Create a new DataFrame with the selected values
selected_data = flood_df.iloc[row_range, col_range]

selected_data

total_ones = (flood_df == 1).sum().sum()
total_ones
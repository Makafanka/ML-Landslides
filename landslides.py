# -*- coding: utf-8 -*-
"""Landslides.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aVkUe6CJnK48xTzWwApLmvILhGiuLH_n
"""

import io
import pandas as pd
import zipfile
import geopandas as gpd
from shapely.geometry import Point
from scipy.spatial import cKDTree
import numpy as np

from google.colab import drive
drive.mount('/content/drive')

# Define the path to your zip file
# zip_file_path = '/content/events.zip'
zip_file_path = '/content/drive/MyDrive/Landslides_Research/events.zip'

# Create an empty list to store DataFrames
dfs = []

# Open the zip file
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    # Iterate through each file in the zip archive
    for file_info in zip_ref.infolist():
        with zip_ref.open(file_info) as file:
            # Read the CSV file into a DataFrame
            df = pd.read_csv(io.TextIOWrapper(file))

            # Extract year and month from the file name
            year, month = file_info.filename.split('_')[1], file_info.filename.split('_')[2].split('.')[0]

            # Add year and month columns to the DataFrame
            df['Year'] = year
            df['Month'] = month

            # Append the DataFrame to the list
            dfs.append(df)

# Concatenate all DataFrames into one
df = pd.concat(dfs, ignore_index=True)

# Now final_df contains all your data from the CSV files with additional 'Year' and 'Month' columns

df['Standardized Type'].unique()

df['Agency-specific Type'].unique()

filt_list1 = ['Incident', 'Obstructions','Flood',
             'Weather Condition', 'Strong Winds', 'Visibility Reduced',
             'tornado', 'Congestion Due to Closure', 'Rain', 'Unknown']
filter1 = df[df['Standardized Type'].isin(filt_list1)]

flood = df[df['Standardized Type'] == 'Flood']
# flood = flood[flood['Agency'] == 'Waze']
flood.shape
print(flood['Agency-specific Type'].unique())

flood.head()

flood.shape

obs = df[df['Standardized Type'] == 'Obstructions']
# obs = obs[obs['Agency'] == 'Waze']
print(obs.shape)
print(obs['Agency-specific Type'].unique())

rain = df[df['Standardized Type'] == 'Rain']
print(rain.shape)
print(rain['Agency-specific Type'].unique())

def spatialJoin(radius, input_data, network):
    # Filter data to include only unpaired points
    cols = ['Event ID', 'Start time', 'Closed time', 'Location', 'Latitude', 'Longitude', 'Road', 'Direction', 'County', 'State', 'Roadway Clearance Time', 'Duration (Incident clearance time)', 'Year', 'Month']
    input_data_todo = input_data[cols].copy()
    print('todo input data shape', input_data_todo.shape)

    # convert points to circles
    input_data_todo = gpd.GeoDataFrame(
        input_data_todo,
        crs={'init': 'epsg:4326'},
        geometry=gpd.points_from_xy(input_data_todo.Longitude, input_data_todo.Latitude))
    input_data_todo_buffer = input_data_todo.to_crs(epsg=3857).buffer(distance=radius, resolution=1).to_crs(epsg=4326)
    input_data_todo['buffer'] = input_data_todo_buffer

    # rename buffer to geometry
    input_data_todo = input_data_todo.rename(columns={'geometry': 'point'})
    input_data_todo = input_data_todo.rename(columns={'buffer': 'geometry'})

    # Filter data to include only unpaired points
    network_todo = network[cols].copy()
    print('todo input data shape', network_todo.shape)
    # convert points to circles
    network_todo = gpd.GeoDataFrame(
        network_todo,
        crs={'init': 'epsg:4326'},
        geometry=gpd.points_from_xy(network_todo.Longitude, network_todo.Latitude))
    network_todo_buffer = network_todo.to_crs(epsg=3857).buffer(distance=radius, resolution=1).to_crs(epsg=4326)
    network_todo['buffer'] = network_todo_buffer

    # rename buffer to geometry
    network_todo = network_todo.rename(columns={'geometry': 'point'})
    network_todo = network_todo.rename(columns={'buffer': 'geometry'})

    # Perform spatial join
    output_data = gpd.sjoin(input_data_todo, network_todo, how='left', op='intersects')

    return output_data

output = spatialJoin(radius=5, input_data=obs, network=flood)
o1 = output
output['join_successful'] = ~output['index_right'].isna()
a = output[output['join_successful'] == True]
print(a)
print(a.shape[0])
output = a

output0 = spatialJoin(radius=5, input_data=obs, network=rain)
o0 = output0
output0['join_successful'] = ~output0['index_right'].isna()
a0 = output0[output0['join_successful'] == True]
print(a0)
print(a0.shape[0])
output0 = a0

output.columns

output['tmc'] = list(zip(output['Event ID_left'], output['Event ID_right']))

# o1['tmc']

output0['tmc'] = list(zip(output0['Event ID_left'], output0['Event ID_right']))

output['Start time_right'] = pd.to_datetime(output['Start time_right'], utc=True)
output['Closed time_right'] = pd.to_datetime(output['Closed time_right'], utc=True)
output['Start time_left'] = pd.to_datetime(output['Start time_left'], utc=True)
output['Closed time_left'] = pd.to_datetime(output['Start time_left'], utc=True)

output0['Start time_right'] = pd.to_datetime(output0['Start time_right'], utc=True)
output0['Closed time_right'] = pd.to_datetime(output0['Closed time_right'], utc=True)
output0['Start time_left'] = pd.to_datetime(output0['Start time_left'], utc=True)
output0['Closed time_left'] = pd.to_datetime(output0['Start time_left'], utc=True)

time_threshold = 7200
temporally_close_events =output[
    (abs(output['Start time_right'] - output['Start time_left']).dt.total_seconds() <= time_threshold) &
    (abs(output['Closed time_right'] - output['Closed time_left']).dt.total_seconds() <= time_threshold)
    ]
temporally_close_events

time_threshold0 = 18000
temporally_close_events0 =output0[
    (abs(output0['Start time_left'] - output0['Start time_right']).dt.total_seconds() <= time_threshold0) &
    (abs(output0['Closed time_left'] - output0['Closed time_right']).dt.total_seconds() <= time_threshold0) &
    (output0['Start time_left'] >= output0['Start time_right']) &
    (output0['Closed time_left'] >= output0['Closed time_right'])
    ]
temporally_close_events0

redundant_rows1 = temporally_close_events.duplicated(subset=['Start time_left', 'Closed time_left'], keep='first')
cleaned_gdf = temporally_close_events[~redundant_rows1]
redundant_rows2 = cleaned_gdf.duplicated(subset=['Latitude_left', 'Longitude_left'], keep='first')
cleaned_gdf = cleaned_gdf[~redundant_rows2]
index_rem = [124574, 513248, 668054, 915953, 1034461, 1168102, 1246667, 1258738, 1273586, 1288782, 1289040, 1510261, 1514016, 1560551, 2155066, 2248922, 2262116, 2279577, 2391860, 2430864, 2506373, 2507208, 2548984, 2561806, 2562409, 2562652, 2714745, 2854288, 2855793, 2983227, 2983695, 3600936, 3630700, 3889753, 4158533, 4172433, 4341266, 4401136, 4420993]
Event_ID = df.iloc[index_rem]['Event ID']
cleaned_gdf = cleaned_gdf[~cleaned_gdf['Event ID_left'].isin(Event_ID)]
cleaned_gdf

cleaned_gdf['point_left'].to_file('drive/MyDrive/Landslides_Research/obs_node.geojson', driver='GeoJSON')
cleaned_gdf['point_right'].to_file('drive/MyDrive/Landslides_Research/flood_node.geojson', driver='GeoJSON')

temporally_close_events.columns

# Save the GeoDataFrame as a GeoJSON file
temporally_close_events['point_left'].to_file('drive/MyDrive/Landslides_Research/obs_j.geojson', driver='GeoJSON')
temporally_close_events['point_right'].to_file('drive/MyDrive/Landslides_Research/flood_j.geojson', driver='GeoJSON')

start_date = pd.Timestamp('2023-01-01', tz='UTC')
end_date = pd.Timestamp('2023-08-31', tz='UTC')

# Create a DatetimeIndex with 30-minute frequency
measurement_tstamp = pd.date_range(start=start_date, end=end_date, freq='30T')

# Print the time stamps
print(measurement_tstamp)

# Create a dictionary to store flood information
event_info = {}

# Iterate over the floods and mark timestamps as flooded
for index, event in output.iterrows():
    start_time_f = event['Start time_right']
    closed_time_f = event['Closed time_right']
    start_time_o = event['Start time_left']
    closed_time_o = event['Closed time_left']
    tmc = event['tmc']

    # if pd.notna(tmc):  # Skip NaN tmcs
    for timestamp in measurement_tstamp:
        if timestamp not in event_info:
            event_info[timestamp] = {}
        if start_time_f <= timestamp <= closed_time_f and start_time_o <= timestamp <= closed_time_o:
            event_info[timestamp][tmc] = 1

# Create a new DataFrame
sorted_items = sorted(event_info.items())
event_info = dict(sorted_items)

event_df = pd.DataFrame(event_info).T.fillna(0).astype(int)
event_df = event_df.reindex(columns=output['tmc'].tolist(), fill_value=0)

event_df

def get_indices_of_same_values(arr, indices):
    unique_values, unique_indices, counts = np.unique(arr, return_inverse=True, return_counts=True)
    indices_dict = {val: [(i, indices[i]) for idx, i in enumerate(np.where(unique_indices == i)[0])]
                    for i, val in enumerate(unique_values) if counts[i] > 1}
    return indices_dict

my_list = np.array(['A', 'B', 'A', 'B', 'B', 'C', 'C', 'D'])
indices = np.array([0, 4, 6, 7, 10, 11, 14, 15])
result = get_indices_of_same_values(my_list, indices)

print(result)

from datetime import datetime

def are_times_near(time1, time2, threshold_seconds):
    format_str = "%H:%M:%S"

    # Convert the time strings to datetime objects
    start_time1 = datetime.strptime(time1[0], format_str)
    end_time1 = datetime.strptime(time1[1], format_str)
    start_time2 = datetime.strptime(time2[0], format_str)
    end_time2 = datetime.strptime(time2[1], format_str)

    # Calculate the time differences in seconds
    time_diff1 = abs((end_time1 - start_time2).total_seconds())
    time_diff2 = abs((end_time2 - start_time1).total_seconds())

    return time_diff1 < threshold_seconds or time_diff2 < threshold_seconds

# Example usage
time1 = ("15:07:29", "15:17:29")
time2 = ("17:18:29", "17:28:29")
threshold_seconds = 600  # 10 minutes

result = are_times_near(time1, time2, threshold_seconds)
print(result)  # Output: True

def check_time(dates, times, indices):
    same_dates = get_indices_of_same_values(dates, indices)
    drop_idx = set()
    for idx_pairs in same_dates.values():
        idx1, ori_idx1 = zip(*idx_pairs)
        times1 = times[idx1]
        prev = times[0]

        for i, t in enumerate(times):
            if i != 0 and are_times_near(prev, t, 1800):
                drop_idx.add(ori_idx1[i])
            else:
                prev = t
    return drop_idx
dates = np.array(["2016-10-13", "2016-10-13"])
times = np.array([("15:46:20", "16:14:41"), ("15:56:12", "22:33:07")])
indices = np.array([0, 2])
print(check_time(dates, times, indices))

def remove_redundant_points(coords, radius, times, dates):
    kdtree = cKDTree(coords)
    indices_within_radius = kdtree.query_ball_point(coords, radius)
    # unique_points = set()
    unique_drop_indices = set()
    for point, indices1 in zip(coords, indices_within_radius):
        dates1 = dates[indices1]
        times1 = times[indices1]
        drop_idx = check_time(dates1, times1, indices1)
        unique_drop_indices.update(drop_idx)
        # unique_points.add(tuple(point))
    # remain_idx = list(set(list_of_indices) - unique_drop_indices)
    remain_idx = np.setdiff1d(np.arange(len(coords)), list(unique_drop_indices))
    return remain_idx
coords = [(38.792469, -77.036253), (39.618915, -76.665476)]
radius = 1
dates = np.array(["2016-10-13", "2016-10-13"])
times = np.array([("15:46:20", "16:14:41"), ("15:56:12", "22:33:07")])
print(remove_redundant_points(coords, radius, times, dates))

coordinates = list(zip(flood['Longitude'], flood['Latitude']))
dates = np.array(flood['Start time'].str.split('T').str[0])
start_t = flood['Start time'].str.split('T').str[1].str.split('-').str[0]
closed_t = flood['Closed time'].str.split('T').str[1].str.split('-').str[0]
times = np.array(list(zip(start_t, closed_t)))
remain_idx = remove_redundant_points(coordinates, radius=2, times=times, dates=dates)

len(remain_idx)

new_flood = flood.iloc[remain_idx]

filt_list2_br = ['Incident', 'Weather Service Alert', 'Weatherhazard',
              'Environment', 'CHART Weatherserviceevent', 'Epd',
              'Brush, Woods, Grass', 'Cavein/trench - Actual/imminent',
              'Other', 'Report', 'Traffic Stop',
              'Slippery Road Event', 'Rain Event', 'Unknown Trouble']
filter2_br = filter1[filter1['Agency-specific Type'].isin(filt_list2_br)]

filt_list2_na = ['Weather Service Alert', 'Weatherhazard', 'Environment',
               'CHART Weatherserviceevent', 'Epd', 'Rain Event']
filter2_na = filter1[filter1['Agency-specific Type'].isin(filt_list2_na)]

print(df.shape)
print(filter2_br.shape)
print(filter2_na.shape)

geometry_br = [Point(lon, lat) for lon, lat in zip(filter2_br['Longitude'], filter2_br['Latitude'])]
gdf_br = gpd.GeoDataFrame(filter2_br, geometry=geometry_br, crs='EPSG:4326')
gdf_br.to_file('drive/MyDrive/Notes/A_filtered_data_broad.geojson', driver='GeoJSON')

geometry_na = [Point(lon, lat) for lon, lat in zip(filter2_na['Longitude'], filter2_na['Latitude'])]
gdf_na = gpd.GeoDataFrame(filter2_na, geometry=geometry_na, crs='EPSG:4326')
gdf_na.to_file('drive/MyDrive/Notes/A_filtered_data_narrow.geojson', driver='GeoJSON')

geometry_f = [Point(lon, lat) for lon, lat in zip(flood['Longitude'], flood['Latitude'])]
gdf_f = gpd.GeoDataFrame(flood, geometry=geometry_f, crs='EPSG:4326')
gdf_f.to_file('drive/MyDrive/Landslides_Research/flood_full.geojson', driver='GeoJSON')

geometry_f = [Point(lon, lat) for lon, lat in zip(obs['Longitude'], obs['Latitude'])]
gdf_f = gpd.GeoDataFrame(obs, geometry=geometry_f, crs='EPSG:4326')
gdf_f.to_file('drive/MyDrive/Landslides_Research/obs_full.geojson', driver='GeoJSON')